import streamlit as st
import re
import jieba
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import io
import pandas as pd

def generate_wordcloud(analysis_text):
    remove_stop_words = st.checkbox('å»é™¤åœç”¨è¯', value=False, key='remove_stop_words_checkbox')
    
    custom_stop_words = st.text_area(
        'è‡ªå®šä¹‰åœç”¨è¯ï¼ˆä½¿ç”¨è‹±æ–‡é€—å· , åˆ†éš”ï¼Œä¾‹å¦‚ï¼šçš„,å’Œ,etc,andï¼‰',
        value='',
        help='è¯·ä½¿ç”¨è‹±æ–‡é€—å·(,)åˆ†éš”æ¯ä¸ªåœç”¨è¯ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆè¾“å…¥',
        key='custom_stop_words_textarea'
    )
    
    try:
        # å°†æ–‡æœ¬ä¸­çš„å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºå•ä¸ªç©ºæ ¼
        text = ' '.join(analysis_text.split())
        
        if remove_stop_words:
            # å¤„ç†åœç”¨è¯åˆ—è¡¨
            stop_words = set(word.strip() for word in custom_stop_words.split(',') if word.strip())
            
            # åˆ†åˆ«å¤„ç†è‹±æ–‡å’Œä¸­æ–‡
            english_words = [word for word in re.findall(r'[A-Za-z]+', text) if word.lower() not in stop_words]
            
            chinese_text = ''.join(re.findall(r'[\u4e00-\u9fff]+', text))
            chinese_words = [word for word in jieba.lcut(chinese_text) if word not in stop_words]
            
            # åˆå¹¶ä¸­è‹±æ–‡è¯é¢‘ç»Ÿè®¡
            all_words = english_words + chinese_words
            word_freq = Counter(all_words)
        else:
            # ä¸å»é™¤è¿æ¥è¯çš„å¤„ç†
            english_words = re.findall(r'[A-Za-z]+', text)
            chinese_text = ''.join(re.findall(r'[\u4e00-\u9fff]+', text))
            chinese_words = jieba.lcut(chinese_text)
            all_words = english_words + chinese_words
            word_freq = Counter(all_words)
        
        # åˆ›å»ºè¯äº‘å›¾
        # æ£€æŸ¥å­—ä½“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        # import os
        # font_path = './static/Hiragino Sans GB.ttc'
        # if not os.path.exists(font_path):
        #     st.warning(f'æœªæ‰¾åˆ°å­—ä½“æ–‡ä»¶: {font_path}ï¼Œè¯äº‘å›¾å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡')
        # # åˆ—å‡º./app/staticç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
        # try:
        #     # è·å–å½“å‰è·¯å¾„
        #     current_path = os.getcwd()
        #     st.write(f'å½“å‰è·¯å¾„: {current_path}')
            
        #     # åˆ—å‡ºå½“å‰è·¯å¾„ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
        #     current_files = os.listdir(current_path)
        #     st.write('å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹:')
        #     for item in current_files:
        #         # åˆ¤æ–­æ˜¯æ–‡ä»¶è¿˜æ˜¯æ–‡ä»¶å¤¹
        #         if os.path.isdir(os.path.join(current_path, item)):
        #             st.write(f'ğŸ“ {item}')
        #         else:
        #             st.write(f'ğŸ“„ {item}')
            
        #     # åˆ—å‡ºstaticç›®å½•ä¸‹çš„æ–‡ä»¶
        #     static_path = './static'
        #     static_files = os.listdir(static_path)
        #     st.write(f'\nstaticç›®å½• ({static_path}) ä¸‹çš„æ–‡ä»¶:')
        #     for file in static_files:
        #         st.write(f'ğŸ“„ {file}')
                
        # except Exception as e:
        #     st.error(f'æ— æ³•è¯»å–ç›®å½•: {str(e)}')
        wc = WordCloud(
            width=1200,
            height=800,
            background_color='white',
            max_words=200,
            font_path='./static/Hiragino Sans GB.ttc',  # ä½¿ç”¨æ”¯æŒä¸­è‹±æ–‡çš„å­—ä½“
            random_state=42
        ).generate_from_frequencies(word_freq)
        
        # æ˜¾ç¤ºè¯äº‘å›¾
        fig, ax = plt.subplots(figsize=(15, 10))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        plt.tight_layout(pad=0)
        
        # åœ¨Streamlitä¸­æ˜¾ç¤ºè¯äº‘å›¾
        st.pyplot(fig)
        
        # æä¾›ä¸‹è½½è¯äº‘å›¾çš„é€‰é¡¹
        img = io.BytesIO()
        plt.savefig(img, format='pdf')
        img.seek(0)
        
        st.download_button(
            label="ä¸‹è½½è¯äº‘å›¾",
            data=img,
            file_name="wordcloud.pdf",
            mime="application/pdf"  # mimeå‚æ•°æŒ‡å®šäº†æ–‡ä»¶çš„MIMEç±»å‹ï¼Œè¿™é‡Œæ˜¯PDFæ–‡ä»¶
        )
        plt.close()
    except Exception as e:
        st.error(f"ç”Ÿæˆè¯äº‘å›¾å¤±è´¥: {str(e)}")
        
        
        
def count_word_frequency(analysis_text):
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        remove_punctuation = st.checkbox('å»é™¤æ ‡ç‚¹ç¬¦å·')
    with col2:
        remove_stopwords = st.checkbox('å»é™¤è¿æ¥è¯')
    with col3:
        remove_numbers = st.checkbox('å»é™¤æ•°å­—')
    with col4:
        top_n = st.number_input('æ˜¾ç¤ºå‰Nä¸ªè¯', min_value=1, value=20)
    words = jieba.lcut(analysis_text)
    
    if remove_punctuation:
        words = [w for w in words if not re.match(r'[^\w]', w)]
        
    if remove_stopwords:
        stopwords = ['çš„','äº†','å’Œ','æ˜¯','åœ¨','æˆ‘','æœ‰','å°±','ä¸','éƒ½','è€Œ','åŠ','ä¸','ç€','æˆ–']
        words = [w for w in words if w not in stopwords]

    if remove_numbers:
        words = [w for w in words if not w.isdigit()]
    
    word_freq = Counter(words)
    freq_df = pd.DataFrame.from_dict(word_freq, orient='index', columns=['é¢‘æ¬¡'])
    freq_df = freq_df.sort_values('é¢‘æ¬¡', ascending=False)
    
    # åªä¿ç•™å‰Nä¸ª
    freq_df = freq_df.head(top_n)
    
    st.write('è¯é¢‘ç»Ÿè®¡ç»“æœ:')
    st.dataframe(freq_df)
    
    # å¯¼å‡ºè¯é¢‘ç»Ÿè®¡ç»“æœ
    csv = freq_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
    st.download_button(
        label="ä¸‹è½½è¯é¢‘ç»Ÿè®¡ç»“æœ", 
        data=csv,
        file_name="word_frequency.csv",
        mime="text/csv"
    )
    
    
def count_characters(analysis_text):
    # ç»Ÿè®¡æ€»å­—æ•°
    char_count = len(analysis_text)
    word_count = len(analysis_text.split())
    
    
    # è®¡ç®—é™¤æ ‡ç‚¹å¤–çš„å­—ç¬¦æ•°
    no_punc_count = len([c for c in analysis_text if not re.match(r'[^\w\s]', c)])
    
    st.write(f'å­—ç¬¦æ•°: {char_count}')
    st.write(f'é™¤æ ‡ç‚¹å¤–çš„å­—ç¬¦æ•°: {no_punc_count}')
    
    # å¯¼å‡ºç»Ÿè®¡ç»“æœ
    stats_dict = {'å­—ç¬¦æ•°': char_count, 'è¯æ•°': word_count}
    stats_df = pd.DataFrame([stats_dict])
    csv = stats_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
    st.download_button(
        label="ä¸‹è½½ç»Ÿè®¡ç»“æœ",
        data=csv,
        file_name="text_statistics.csv",
        mime="text/csv"
    )

def split_words(analysis_text):
    st.subheader('åˆ†è¯ç»“æœ')
    
    try:
        # å°†æ–‡æœ¬ä¸­çš„å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºå•ä¸ªç©ºæ ¼
        text = ' '.join(analysis_text.split())
        
        # åˆ†åˆ«å¤„ç†è‹±æ–‡å’Œä¸­æ–‡
        # è‹±æ–‡ï¼šæŒ‰ç©ºæ ¼åˆ†è¯ï¼Œä¿ç•™æ ‡ç‚¹
        english_pattern = r'[A-Za-z]+(?:\'[A-Za-z]+)*|[.,!?;]'
        english_words = re.findall(english_pattern, text)
        
        # ä¸­æ–‡ï¼šä½¿ç”¨jiebaåˆ†è¯
        chinese_text = ''.join(re.findall(r'[\u4e00-\u9fff]+', text))
        chinese_words = jieba.lcut(chinese_text)
        
        # åˆå¹¶ç»“æœ
        all_words = english_words + chinese_words
        
        # æ˜¾ç¤ºåˆ†è¯ç»“æœ
        st.text_area(
            'åˆ†è¯ç»“æœï¼ˆè¯è¯­é—´ä»¥ç©ºæ ¼åˆ†éš”ï¼‰ï¼š',
            value=' '.join(all_words),
            height=200,
            key='split_words_result'
        )
        
        # æä¾›ä¸‹è½½é€‰é¡¹
        result_str = ' '.join(all_words)
        st.download_button(
            label="ä¸‹è½½åˆ†è¯ç»“æœ",
            data=result_str.encode('utf-8'),
            file_name="split_words_result.txt",
            mime="text/plain"
        )
        
    except Exception as e:
        st.error(f"åˆ†è¯å¤±è´¥: {str(e)}")
