import streamlit as st
import re
import jieba
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import io
import pandas as pd

def generate_wordcloud(analysis_text):
    remove_stop_words = st.checkbox('å»é™¤åœç”¨è¯', value=False, key='remove_stop_words_checkbox')
    
    custom_stop_words = st.text_area(
        'è‡ªå®šä¹‰åœç”¨è¯ï¼ˆä½¿ç”¨è‹±æ–‡é€—å· , åˆ†éš”ï¼Œä¾‹å¦‚ï¼šçš„,å’Œ,etc,andï¼‰',
        value='',
        help='è¯·ä½¿ç”¨è‹±æ–‡é€—å·(,)åˆ†éš”æ¯ä¸ªåœç”¨è¯ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆè¾“å…¥',
        key='custom_stop_words_textarea'
    )
    
    try:
        # å°†æ–‡æœ¬ä¸­çš„å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºå•ä¸ªç©ºæ ¼
        text = ' '.join(analysis_text.split())
        
        if remove_stop_words:
            # æ›´æ–°åœç”¨è¯åˆ—è¡¨
            stop_words = set(word.strip() for word in custom_stop_words.split(',') if word.strip())
            stop_words.update(['æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬', 
                               'çš„', 'äº†', 'å’Œ', 'åœ¨', 'æ˜¯', 'ä¸', 'ä¹Ÿ', 'æœ‰', 'å¯¹', 'åˆ°', 'è¯´', 
                               'çœ‹', 'å¾ˆ', 'éƒ½', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'å°±', 'äºº', 'å› ä¸º', 'æ€ä¹ˆ', 
                               'ä¸€ä¸ª', 'è€Œ', 'ä½†', 'ä¼š', 'èƒ½', 'è®©', 'å¦‚æœ', 'åˆ', 'ç”¨', 'è‡ªå·±', 
                               'å¤š', 'æ²¡', 'ä¸º', 'å»', 'ç„¶å', 'è¿™æ ·', 'é‚£æ ·', 'çœŸçš„', 'æ‰€ä»¥', 
                               'å…¶å®', 'å¹¶', 'å§', 'å—', 'å‘¢', 'å°±æ˜¯', 'è€Œä¸”', 'æˆ–è€…', 'å¯ä»¥', 
                               'å¯èƒ½', 'åƒ', 'è¦', 'æ¯”å¦‚', 'ä»', 'æ›´', 'è¿™å„¿', 'é‚£å„¿', 'é‚£ä¹ˆ','ç­‰','å¦‚æ­¤'])
            
            # åˆ†åˆ«å¤„ç†è‹±æ–‡å’Œä¸­æ–‡
            english_words = [word for word in re.findall(r'[A-Za-z]+', text) if word.lower() not in stop_words]
            
            chinese_text = ''.join(re.findall(r'[\u4e00-\u9fff]+', text))
            chinese_words = [word for word in jieba.lcut(chinese_text) if word not in stop_words]
            
            # åˆå¹¶ä¸­è‹±æ–‡è¯é¢‘ç»Ÿè®¡
            all_words = english_words + chinese_words
            word_freq = Counter(all_words)
        else:
            # ä¸å»é™¤è¿æ¥è¯çš„å¤„ç†
            english_words = re.findall(r'[A-Za-z]+', text)
            chinese_text = ''.join(re.findall(r'[\u4e00-\u9fff]+', text))
            chinese_words = jieba.lcut(chinese_text)
            all_words = english_words + chinese_words
            word_freq = Counter(all_words)
        
        # åˆ›å»ºè¯äº‘å›¾
        wc = WordCloud(
            width=1200,
            height=800,
            background_color='white',
            max_words=200,
            font_path='./static/Hiragino Sans GB.ttc',  # ä½¿ç”¨æ”¯æŒä¸­è‹±æ–‡çš„å­—ä½“
            random_state=42
        ).generate_from_frequencies(word_freq)
        
        # æ˜¾ç¤ºè¯äº‘å›¾
        fig, ax = plt.subplots(figsize=(15, 10))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        plt.tight_layout(pad=0)
        
        # åœ¨Streamlitä¸­æ˜¾ç¤ºè¯äº‘å›¾
        st.pyplot(fig)
        
        # æä¾›ä¸‹è½½è¯äº‘å›¾çš„é€‰é¡¹
        img = io.BytesIO()
        plt.savefig(img, format='pdf')
        img.seek(0)
        
        st.download_button(
            label="ä¸‹è½½è¯äº‘å›¾",
            data=img,
            file_name="wordcloud.pdf",
            mime="application/pdf"  # mimeå‚æ•°æŒ‡å®šäº†æ–‡ä»¶çš„MIMEç±»å‹ï¼Œè¿™é‡Œæ˜¯PDFæ–‡ä»¶
        )
        plt.close()
    except Exception as e:
        st.error(f"ç”Ÿæˆè¯äº‘å›¾å¤±è´¥: {str(e)}")
        
def count_word_frequency(analysis_text):
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        remove_punctuation = st.checkbox('å»é™¤æ ‡ç‚¹ç¬¦å·')
    with col2:
        remove_stopwords = st.checkbox('å»é™¤åœç”¨è¯')
    with col3:
        remove_numbers = st.checkbox('å»é™¤æ•°å­—')
    with col4:
        top_n = st.number_input('æ˜¾ç¤ºå‰Nä¸ªè¯', min_value=1, value=20)
    words = jieba.lcut(analysis_text)
    
    if remove_punctuation:
        words = [w for w in words if not re.match(r'[^\w]', w)]
        
    if remove_stopwords:
        stopwords = ['æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬', 
                     'çš„', 'äº†', 'å’Œ', 'åœ¨', 'æ˜¯', 'ä¸', 'ä¹Ÿ', 'æœ‰', 'å¯¹', 'åˆ°', 'è¯´', 
                     'çœ‹', 'å¾ˆ', 'éƒ½', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'å°±', 'äºº', 'å› ä¸º', 'æ€ä¹ˆ', 
                     'ä¸€ä¸ª', 'è€Œ', 'ä½†', 'ä¼š', 'èƒ½', 'è®©', 'å¦‚æœ', 'åˆ', 'ç”¨', 'è‡ªå·±', 
                     'å¤š', 'æ²¡', 'ä¸º', 'å»', 'ç„¶å', 'è¿™æ ·', 'é‚£æ ·', 'çœŸçš„', 'æ‰€ä»¥', 
                     'å…¶å®', 'å¹¶', 'å§', 'å—', 'å‘¢', 'å°±æ˜¯', 'è€Œä¸”', 'æˆ–è€…', 'å¯ä»¥', 
                     'å¯èƒ½', 'åƒ', 'è¦', 'æ¯”å¦‚', 'ä»', 'æ›´', 'è¿™å„¿', 'é‚£å„¿', 'é‚£ä¹ˆ','ç­‰','å¦‚æ­¤']
        words = [w for w in words if w not in stopwords]
    
    if remove_numbers:
        words = [w for w in words if not w.isdigit()]
    
    word_freq = Counter(words)
    freq_df = pd.DataFrame.from_dict(word_freq, orient='index', columns=['é¢‘æ¬¡'])
    freq_df = freq_df.sort_values('é¢‘æ¬¡', ascending=False)
    
    # åªä¿ç•™å‰Nä¸ª
    freq_df = freq_df.head(top_n)
    
    st.write('è¯é¢‘ç»Ÿè®¡ç»“æœ:')
    st.dataframe(freq_df)
    
    # å¯¼å‡ºè¯é¢‘ç»Ÿè®¡ç»“æœ
    csv = freq_df.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
    st.download_button(
        label="ä¸‹è½½è¯é¢‘ç»Ÿè®¡ç»“æœ", 
        data=csv,
        file_name="word_frequency.csv",
        mime="text/csv"
    )
    
    
def count_characters(analysis_text):
    # 1. æ€»å­—ç¬¦æ•°ï¼ˆä¸åŒ…æ‹¬ç©ºæ ¼å’Œæ¢è¡Œç¬¦ï¼‰
    char_count_no_space = len([c for c in analysis_text if not c.isspace()])
    
    # 2. æœ‰æ•ˆå­—ç¬¦æ•°ï¼ˆä¸åŒ…æ‹¬ç©ºæ ¼ã€æ¢è¡Œç¬¦å’Œæ ‡ç‚¹ç¬¦å·ï¼‰
    valid_chars = len([c for c in analysis_text if c.isalnum() or '\u4e00' <= c <= '\u9fff'])
    
    # 3. åˆ†ç±»ç»Ÿè®¡
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', analysis_text))
    english_chars = len(re.findall(r'[a-zA-Z]', analysis_text))
    numbers = len(re.findall(r'\d', analysis_text))
    spaces = len(re.findall(r'\s', analysis_text))
    punctuation = len([c for c in analysis_text if re.match(r'[^\w\s]', c)])
    
    # 4. è¯æ•°ç»Ÿè®¡
    # è‹±æ–‡è¯æ•°
    english_words = len([word for word in analysis_text.split() if re.match(r'[a-zA-Z]+', word)])
    # ä¸­æ–‡è¯æ•°ï¼ˆä½¿ç”¨jiebaåˆ†è¯ï¼‰
    chinese_text = ''.join(re.findall(r'[\u4e00-\u9fff]+', analysis_text))
    chinese_words = len(jieba.lcut(chinese_text))
    total_words = english_words + chinese_words
    
    # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
    st.write("### å­—ç¬¦ç»Ÿè®¡")
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**åŸºæœ¬ç»Ÿè®¡ï¼š**")
        st.write(f'æ€»å­—ç¬¦æ•°ï¼ˆåŒ…å«æ‰€æœ‰å­—ç¬¦ï¼‰: {len(analysis_text)}')
        st.write(f'æ€»å­—ç¬¦æ•°ï¼ˆä¸å«ç©ºç™½å­—ç¬¦ï¼‰: {char_count_no_space}')
        st.write(f'æœ‰æ•ˆå­—ç¬¦æ•°ï¼ˆä»…å­—æ¯ã€æ•°å­—ã€æ±‰å­—ï¼‰: {valid_chars}')
        st.write(f'æ€»è¯æ•°: {total_words}')
    
    with col2:
        st.markdown("**è¯¦ç»†åˆ†ç±»ï¼š**")
        st.write(f'ä¸­æ–‡å­—ç¬¦æ•°: {chinese_chars}')
        st.write(f'è‹±æ–‡å­—ç¬¦æ•°: {english_chars}')
        st.write(f'æ•°å­—ä¸ªæ•°: {numbers}')
        st.write(f'ç©ºæ ¼åŠæ¢è¡Œæ•°: {spaces}')
        st.write(f'æ ‡ç‚¹ç¬¦å·æ•°: {punctuation}')
    
    # å¯¼å‡ºç»Ÿè®¡ç»“æœ
    stats_dict = {
        'æ€»å­—ç¬¦æ•°ï¼ˆå…¨éƒ¨ï¼‰': len(analysis_text),
        'æ€»å­—ç¬¦æ•°ï¼ˆä¸å«ç©ºç™½ï¼‰': char_count_no_space,
        'æœ‰æ•ˆå­—ç¬¦æ•°': valid_chars,
        'æ€»è¯æ•°': total_words,
        'ä¸­æ–‡å­—ç¬¦æ•°': chinese_chars,
        'è‹±æ–‡å­—ç¬¦æ•°': english_chars,
        'æ•°å­—ä¸ªæ•°': numbers,
        'ç©ºæ ¼åŠæ¢è¡Œæ•°': spaces,
        'æ ‡ç‚¹ç¬¦å·æ•°': punctuation,
        'ä¸­æ–‡è¯æ•°': chinese_words,
        'è‹±æ–‡è¯æ•°': english_words
    }
    stats_df = pd.DataFrame([stats_dict])
    csv = stats_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
    st.download_button(
        label="ä¸‹è½½ç»Ÿè®¡ç»“æœ",
        data=csv,
        file_name="text_statistics.csv",
        mime="text/csv"
    )

def split_words(analysis_text):
    st.subheader('åˆ†è¯ç»“æœ')
    
    try:
        # å°†æ–‡æœ¬ä¸­çš„å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºå•ä¸ªç©ºæ ¼
        text = ' '.join(analysis_text.split())
        
        # åˆ†åˆ«å¤„ç†è‹±æ–‡å’Œä¸­æ–‡
        # è‹±æ–‡ï¼šæŒ‰ç©ºæ ¼åˆ†è¯ï¼Œä¿ç•™æ ‡ç‚¹
        english_pattern = r'[A-Za-z]+(?:\'[A-Za-z]+)*|[.,!?;]'
        english_words = re.findall(english_pattern, text)
        
        # ä¸­æ–‡ï¼šä½¿ç”¨jiebaåˆ†è¯
        chinese_text = ''.join(re.findall(r'[\u4e00-\u9fff]+', text))
        chinese_words = jieba.lcut(chinese_text)
        
        # åˆå¹¶ç»“æœ
        all_words = english_words + chinese_words
        
        # æ˜¾ç¤ºåˆ†è¯ç»“æœ
        st.text_area(
            'åˆ†è¯ç»“æœï¼ˆè¯è¯­é—´ä»¥ç©ºæ ¼åˆ†éš”ï¼‰ï¼š',
            value=' '.join(all_words),
            height=200,
            key='split_words_result'
        )
        
        # æä¾›ä¸‹è½½é€‰é¡¹
        result_str = ' '.join(all_words)
        st.download_button(
            label="ä¸‹è½½åˆ†è¯ç»“æœ",
            data=result_str.encode('utf-8'),
            file_name="split_words_result.txt",
            mime="text/plain"
        )
        
    except Exception as e:
        st.error(f"åˆ†è¯å¤±è´¥: {str(e)}")

def text_annotation(text):
    """æ–‡æœ¬æ ‡æ³¨åŠŸèƒ½"""
    if not text:
        st.warning('è¯·å…ˆè¾“å…¥è¦æ ‡æ³¨çš„æ–‡æœ¬')
        return
    
    # ä½¿ç”¨session_stateå­˜å‚¨å¤„ç†åçš„æ–‡æœ¬
    if 'processed_text' not in st.session_state:
        st.session_state.processed_text = text
    
    # æ–‡æœ¬é¢„å¤„ç†é€‰é¡¹
    st.write("### æ–‡æœ¬é¢„å¤„ç†é€‰é¡¹")
    col1, col2, col3 = st.columns(3)
    with col1:
        remove_punctuation = st.checkbox('å»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™å¥å·ï¼‰', key='annotation_remove_punct')
    with col2:
        remove_spaces = st.checkbox('å»é™¤å¤šä½™ç©ºæ ¼', key='annotation_remove_space')
    with col3:
        remove_numbers = st.checkbox('å»é™¤æ•°å­—', key='annotation_remove_num')
    
    # æ›´æ–°åˆ†è¯å¤„ç†
    def process_text(text):
        # å…ˆå¤„ç†è‹±æ–‡å•è¯
        # åœ¨è‹±æ–‡å•è¯ä¹‹é—´æ·»åŠ ç©ºæ ¼
        text = re.sub(r'([a-zA-Z])([A-Z])', r'\1 \2', text)  # å¤„ç†é©¼å³°å‘½å
        text = re.sub(r'([a-zA-Z])(\d)', r'\1 \2', text)     # å¤„ç†å­—æ¯å’Œæ•°å­—
        text = re.sub(r'(\d)([a-zA-Z])', r'\1 \2', text)     # å¤„ç†æ•°å­—å’Œå­—æ¯
        
        # ä½¿ç”¨jiebaåˆ†è¯ï¼Œä½†ä¿ç•™è‹±æ–‡å•è¯çš„å®Œæ•´æ€§
        words = []
        for segment in text.split():  # å…ˆæŒ‰ç©ºæ ¼åˆ†å‰²
            if re.match(r'^[a-zA-Z]+$', segment):  # å¦‚æœæ˜¯çº¯è‹±æ–‡å•è¯
                words.append(segment)
            else:  # å¯¹éè‹±æ–‡éƒ¨åˆ†ä½¿ç”¨jiebaåˆ†è¯
                words.extend(jieba.lcut(segment))
        return words

    # å¤„ç†æ–‡æœ¬
    if text:
        st.session_state.processed_text = text
        words = process_text(text)
        st.session_state.words = words
        
    # æ–‡æœ¬é¢„å¤„ç†
    if st.button("åº”ç”¨é¢„å¤„ç†", key='apply_preprocessing'):
        processed_text = text
        if remove_punctuation:
            # ä¿ç•™å¥å·ã€æ„Ÿå¹å·ã€é—®å·ç­‰åˆ†å¥æ ‡ç‚¹
            processed_text = re.sub(r'[^\w\sã€‚ï¼ï¼Ÿ!?.]', '', processed_text)
        if remove_spaces:
            # åˆå¹¶å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼å¹¶ç§»é™¤è‹±æ–‡å•è¯ä¹‹é—´çš„ç©ºæ ¼
            words = processed_text.split()
            processed_text = ''.join(words)
        if remove_numbers:
            processed_text = re.sub(r'\d+', '', processed_text)
        
        # ä¿å­˜å¤„ç†åçš„æ–‡æœ¬
        st.session_state.processed_text = processed_text
        st.success('é¢„å¤„ç†å®Œæˆï¼')
    
    # æ˜¾ç¤ºå½“å‰è¦å¤„ç†çš„æ–‡æœ¬
    st.write("### å½“å‰æ–‡æœ¬")
    st.text_area("æ–‡æœ¬å†…å®¹ï¼š", st.session_state.processed_text, height=100, key='current_text_display')
    
    # åˆ†å¥
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?])', st.session_state.processed_text)
    sentences = [''.join(i) for i in zip(sentences[0::2], sentences[1::2] + [''])]
    sentences = [s.strip() for s in sentences if s.strip()]
    
    # ä½¿ç”¨infoæ˜¾ç¤ºåˆ†å¥ç»“æœ
    st.info(f"âœ‚ï¸ æ–‡æœ¬å·²è¢«åˆ†å‰²ä¸º {len(sentences)} ä¸ªå¥å­")
    
    # ä½¿ç”¨æ˜æ˜¾çš„åˆ†éš”çº¿å’Œæ ·å¼çªå‡ºæ ‡æ³¨æ¨¡å¼é€‰æ‹©
    st.markdown("---")
    st.markdown("""
    <style>
    .annotation-header {
        font-size: 1.5em;
        font-weight: bold;
        color: #FF4B4B;
        padding: 10px 0;
        margin: 20px 0;
    }
    </style>
    <div class="annotation-header">ğŸ“ é€‰æ‹©æ ‡æ³¨æ¨¡å¼</div>
    """, unsafe_allow_html=True)
    
    # æ ‡æ³¨æ¨¡å¼é€‰æ‹©
    annotation_mode = st.radio(
        "",  # ç§»é™¤æ ‡ç­¾æ–‡å­—ï¼Œå› ä¸ºå·²ç»ç”¨ä¸Šé¢çš„æ ‡é¢˜æ›¿ä»£
        ["è¯è¯­çº§æ ‡æ³¨ï¼ˆæ ‡æ³¨æ¯ä¸ªè¯çš„ç±»åˆ«ï¼‰", "å¥å­çº§æ ‡æ³¨ï¼ˆæ ‡æ³¨æ•´å¥çš„ç±»åˆ«ï¼‰"],
        key="annotation_mode_select"
    )
    
    st.markdown("---")  # æ·»åŠ åˆ†éš”çº¿
    
    if annotation_mode == "è¯è¯­çº§æ ‡æ³¨ï¼ˆæ ‡æ³¨æ¯ä¸ªè¯çš„ç±»åˆ«ï¼‰":
        # æ ‡æ³¨ç±»å‹é€‰æ‹©
        st.markdown('<div style="font-size: 1.2em; font-weight: bold; color: #333;">é€‰æ‹©æ ‡æ³¨ç±»å‹ï¼š</div>', 
                   unsafe_allow_html=True)
        label_type = st.radio(
            "",  # ç§»é™¤æ ‡ç­¾æ–‡å­—
            ["å‘½åå®ä½“", "è¯æ€§", "è¯­ä¹‰è§’è‰²", "è‡ªå®šä¹‰æ ‡æ³¨"],
            key="label_type_select"
        )
        
        # æ ¹æ®ä¸åŒçš„æ ‡æ³¨ç±»å‹æä¾›ä¸åŒçš„é»˜è®¤æ ‡ç­¾
        if label_type == "å‘½åå®ä½“":
            default_labels = "äººå,åœ°å,ç»„ç»‡å,æ—¶é—´,æ•°é‡,å…¶ä»–"
            help_text = "ç”¨äºæ ‡æ³¨æ–‡æœ¬ä¸­çš„å®ä½“ç±»å‹"
        elif label_type == "è¯æ€§":
            default_labels = "åè¯,åŠ¨è¯,å½¢å®¹è¯,å‰¯è¯,ä»£è¯,ä»‹è¯,è¿è¯,åŠ©è¯,å¹è¯,æ•°è¯,é‡è¯,å…¶ä»–"
            help_text = "ç”¨äºæ ‡æ³¨è¯è¯­çš„è¯æ€§ç±»åˆ«"
        elif label_type == "è¯­ä¹‰è§’è‰²":
            default_labels = "æ–½äº‹,å—äº‹,ä¸äº‹,å·¥å…·,å¤„æ‰€,æ—¶é—´,æ–¹å¼,åŸå› ,ç›®çš„,ç»“æœ,å…¶ä»–"
            help_text = "ç”¨äºæ ‡æ³¨è¯è¯­åœ¨å¥å­ä¸­çš„è¯­ä¹‰è§’è‰²"
        else:  # è‡ªå®šä¹‰æ ‡æ³¨
            default_labels = "æ ‡ç­¾1,æ ‡ç­¾2,æ ‡ç­¾3"
            help_text = "è¯·è¾“å…¥æ‚¨è‡ªå®šä¹‰çš„æ ‡æ³¨ç±»åˆ«ï¼Œç”¨é€—å·åˆ†éš”"
            
            # è‡ªå®šä¹‰æ ‡æ³¨åç§°
            custom_annotation_name = st.text_input(
                "è¾“å…¥æ ‡æ³¨ä»»åŠ¡åç§°ï¼ˆä¾‹å¦‚ï¼šæƒ…æ„Ÿå€¾å‘ã€ä¸»é¢˜åˆ†ç±»ç­‰ï¼‰ï¼š",
                value="è‡ªå®šä¹‰æ ‡æ³¨ä»»åŠ¡",
                key="custom_annotation_name"
            )
            st.write(f"å½“å‰æ ‡æ³¨ä»»åŠ¡ï¼š{custom_annotation_name}")
        
        # è‡ªå®šä¹‰æ ‡ç­¾
        st.write(f"è®¾ç½®{label_type}æ ‡æ³¨çš„ç±»åˆ«")
        custom_labels = st.text_input(
            "è¾“å…¥æ ‡æ³¨ç±»åˆ«ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼š",
            value=default_labels,
            help=help_text,
            key="annotation_custom_labels"
        ).split(',')
        
        # æ˜¾ç¤ºæ ‡æ³¨è¯´æ˜
        with st.expander("æŸ¥çœ‹æ ‡æ³¨è¯´æ˜"):
            if label_type == "å‘½åå®ä½“":
                st.markdown("""
                - äººåï¼šäººç‰©çš„å§“å
                - åœ°åï¼šåœ°ç†ä½ç½®åç§°
                - ç»„ç»‡åï¼šå…¬å¸ã€æœºæ„ã€ç»„ç»‡ç­‰åç§°
                - æ—¶é—´ï¼šæ—¶é—´è¯è¯­
                - æ•°é‡ï¼šæ•°é‡è¯è¯­
                - å…¶ä»–ï¼šå…¶ä»–ç±»å‹çš„å®ä½“
                """)
            elif label_type == "è¯æ€§":
                st.markdown("""
                - åè¯ï¼šè¡¨ç¤ºäººã€äº‹ç‰©ã€æ¦‚å¿µç­‰
                - åŠ¨è¯ï¼šè¡¨ç¤ºåŠ¨ä½œã€è¡Œä¸ºã€å˜åŒ–ç­‰
                - å½¢å®¹è¯ï¼šè¡¨ç¤ºæ€§è´¨ã€çŠ¶æ€ç­‰
                - å‰¯è¯ï¼šä¿®é¥°åŠ¨è¯ã€å½¢å®¹è¯ç­‰
                - ä»£è¯ï¼šä»£æ›¿åè¯ã€æ•°è¯ç­‰
                - ä»‹è¯ï¼šè¡¨ç¤ºå…³ç³»çš„è™šè¯
                - è¿è¯ï¼šè¿æ¥è¯è¯­ã€å¥å­çš„è™šè¯
                - åŠ©è¯ï¼šè¾…åŠ©è¡¨è¾¾çš„è™šè¯
                - å¹è¯ï¼šè¡¨ç¤ºæ„Ÿå¹è¯­æ°”
                - æ•°è¯ï¼šè¡¨ç¤ºæ•°é‡
                - é‡è¯ï¼šè¡¨ç¤ºå•ä½
                - å…¶ä»–ï¼šå…¶ä»–è¯æ€§
                """)
            elif label_type == "è¯­ä¹‰è§’è‰²":
                st.markdown("""
                - æ–½äº‹ï¼šåŠ¨ä½œçš„æ‰§è¡Œè€…
                - å—äº‹ï¼šåŠ¨ä½œçš„æ‰¿å—è€…
                - ä¸äº‹ï¼šåŠ¨ä½œæ¶‰åŠçš„å…¶ä»–å¯¹è±¡
                - å·¥å…·ï¼šåŠ¨ä½œä½¿ç”¨çš„å·¥å…·
                - å¤„æ‰€ï¼šåŠ¨ä½œå‘ç”Ÿçš„åœ°ç‚¹
                - æ—¶é—´ï¼šåŠ¨ä½œå‘ç”Ÿçš„æ—¶é—´
                - æ–¹å¼ï¼šåŠ¨ä½œçš„æ–¹å¼
                - åŸå› ï¼šåŠ¨ä½œçš„åŸå› 
                - ç›®çš„ï¼šåŠ¨ä½œçš„ç›®çš„
                - ç»“æœï¼šåŠ¨ä½œçš„ç»“æœ
                - å…¶ä»–ï¼šå…¶ä»–è¯­ä¹‰è§’è‰²
                """)
            else:  # è‡ªå®šä¹‰æ ‡æ³¨è¯´æ˜
                st.markdown("""
                ### è‡ªå®šä¹‰æ ‡æ³¨è¯´æ˜
                1. åœ¨ä¸Šæ–¹è¾“å…¥æ‚¨çš„æ ‡æ³¨ä»»åŠ¡åç§°
                2. åœ¨æ ‡æ³¨ç±»åˆ«ä¸­è¾“å…¥æ‚¨éœ€è¦çš„æ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”
                3. å»ºè®®æ·»åŠ "å…¶ä»–"ç±»åˆ«ä»¥å¤„ç†ç‰¹æ®Šæƒ…å†µ
                4. æ ‡ç­¾åç§°å»ºè®®ç®€æ´æ˜ç¡®
                5. å¯ä»¥åœ¨ä¸‹æ–¹æ·»åŠ æ‚¨çš„æ ‡æ³¨è§„åˆ™è¯´æ˜
                """)
                
                # å…è®¸ç”¨æˆ·æ·»åŠ è‡ªå®šä¹‰è¯´æ˜
                custom_guidelines = st.text_area(
                    "æ·»åŠ æ‚¨çš„æ ‡æ³¨è§„åˆ™è¯´æ˜ï¼ˆå¯é€‰ï¼‰ï¼š",
                    value="",
                    height=100,
                    key="custom_guidelines"
                )
                if custom_guidelines:
                    st.markdown("### è‡ªå®šä¹‰æ ‡æ³¨è§„åˆ™")
                    st.markdown(custom_guidelines)
        
        # åˆå§‹åŒ–æ ‡æ³¨ç»“æœ
        if 'annotations' not in st.session_state:
            st.session_state.annotations = {}
        
        # æ˜¾ç¤ºæ ‡æ³¨ç•Œé¢
        st.write("### è¯è¯­æ ‡æ³¨")
        for i, sentence in enumerate(sentences):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"å¥å­ {i+1}: {sentence}")
            with col2:
                # æ”¹è¿›çš„è‹±æ–‡åˆ†è¯å¤„ç†
                def split_english_words(text):
                    # å¤„ç†é©¼å³°å‘½å
                    text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)
                    # å¤„ç†è¿ç»­çš„å¤§å†™å­—æ¯ï¼ˆå¦‚AIã€GPUï¼‰
                    text = re.sub(r'([A-Z])([A-Z][a-z])', r'\1 \2', text)
                    # å¤„ç†æ•°å­—å’Œå­—æ¯çš„ç»„åˆ
                    text = re.sub(r'([a-zA-Z])(\d)', r'\1 \2', text)
                    text = re.sub(r'(\d)([a-zA-Z])', r'\1 \2', text)
                    return text

                # å…ˆå¤„ç†è‹±æ–‡
                processed_sentence = split_english_words(sentence)
                
                # åˆ†è¯å¤„ç†
                words = []
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰¾å‡ºæ‰€æœ‰è‹±æ–‡å•è¯å’Œå…¶ä»–å­—ç¬¦
                pattern = r'[A-Za-z]+|[\u4e00-\u9fff]+'
                matches = re.finditer(pattern, processed_sentence)
                
                for match in matches:
                    word = match.group()
                    if re.match(r'^[A-Za-z]+$', word):  # è‹±æ–‡å•è¯
                        words.append(word)
                    else:  # ä¸­æ–‡å­—ç¬¦
                        words.extend(jieba.lcut(word))
                
                # æ ‡æ³¨å¤„ç†
                annotations = []
                for j, word in enumerate(words):
                    if word.strip():  # åªæ ‡æ³¨éç©ºè¯è¯­
                        unique_key = f"annotation_seq_{i}_{j}_{word}"
                        label = st.selectbox(
                            f"'{word}' çš„ç±»åˆ«",
                            ["æ— æ ‡æ³¨"] + custom_labels,
                            key=unique_key
                        )
                        annotations.append((word, label))
                st.session_state.annotations[i] = annotations
                
    else:  # å¥å­çº§æ ‡æ³¨
        # è‡ªå®šä¹‰ç±»åˆ«
        st.write("è®¾ç½®å¥å­æ ‡æ³¨çš„ç±»åˆ«ï¼Œä¾‹å¦‚ï¼šç§¯æã€æ¶ˆæã€ä¸­æ€§ç­‰")
        custom_categories = st.text_input(
            "è¾“å…¥æ ‡æ³¨ç±»åˆ«ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼š",
            value="ç§¯æ,æ¶ˆæ,ä¸­æ€§",
            help="è¿™äº›ç±»åˆ«å°†ç”¨äºæ ‡æ³¨æ•´ä¸ªå¥å­çš„å±æ€§",
            key="annotation_custom_categories"
        ).split(',')
        
        # åˆå§‹åŒ–åˆ†ç±»ç»“æœ
        if 'classifications' not in st.session_state:
            st.session_state.classifications = {}
            
        # æ˜¾ç¤ºåˆ†ç±»ç•Œé¢
        st.write("### å¥å­æ ‡æ³¨")
        for i, sentence in enumerate(sentences):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"å¥å­ {i+1}: {sentence}")
            with col2:
                category = st.selectbox(
                    "é€‰æ‹©å¥å­ç±»åˆ«",
                    custom_categories,
                    key=f"annotation_cat_{i}"
                )
                st.session_state.classifications[i] = {
                    'text': sentence,
                    'category': category
                }
    
    # å¯¼å‡ºæ ‡æ³¨ç»“æœ
    if st.button('å¯¼å‡ºæ ‡æ³¨ç»“æœ', key='annotation_export'):
        if annotation_mode == "è¯è¯­çº§æ ‡æ³¨ï¼ˆæ ‡æ³¨æ¯ä¸ªè¯çš„ç±»åˆ«ï¼‰":
            # æ”¶é›†æ‰€æœ‰æ ‡æ³¨ç»“æœ
            results = []
            for sent_id, annotations in st.session_state.annotations.items():
                for word, label in annotations:
                    if label != "æ— æ ‡æ³¨":  # åªæ”¶é›†å·²æ ‡æ³¨çš„è¯
                        results.append({
                            'sentence_id': sent_id + 1,
                            'word': word,
                            'label': label
                        })
            
            # åˆ›å»ºå®Œæ•´æ•°æ®å’Œå·²æ ‡æ³¨æ•°æ®çš„DataFrame
            df_all = pd.DataFrame([
                {
                    'sentence_id': sent_id + 1,
                    'word': word,
                    'label': label
                }
                for sent_id, annotations in st.session_state.annotations.items()
                for word, label in annotations
            ])
            
            df_labeled = pd.DataFrame(results)
            
            col1, col2 = st.columns(2)
            
            with col1:
                # å¯¼å‡ºå…¨éƒ¨æ•°æ®
                if not df_all.empty:
                    csv_all = df_all.to_csv(index=False).encode('utf-8-sig')
                    st.download_button(
                        label="ä¸‹è½½å…¨éƒ¨æ ‡æ³¨æ•°æ®(CSV)",
                        data=csv_all,
                        file_name="annotations_all.csv",
                        mime="text/csv",
                        help="åŒ…å«æ‰€æœ‰è¯è¯­çš„æ ‡æ³¨ç»“æœï¼ŒåŒ…æ‹¬æœªæ ‡æ³¨çš„è¯"
                    )
            
            with col2:
                # å¯¼å‡ºå·²æ ‡æ³¨æ•°æ®
                if not df_labeled.empty:
                    csv_labeled = df_labeled.to_csv(index=False).encode('utf-8-sig')
                    st.download_button(
                        label="ä¸‹è½½å·²æ ‡æ³¨æ•°æ®(CSV)",
                        data=csv_labeled,
                        file_name="annotations_labeled.csv",
                        mime="text/csv",
                        help="åªåŒ…å«å·²æ ‡æ³¨çš„è¯è¯­ï¼ˆä¸åŒ…å«"æ— æ ‡æ³¨"çš„è¯ï¼‰"
                    )
            
            # æ˜¾ç¤ºæ ‡æ³¨ç»Ÿè®¡ä¿¡æ¯
            st.info(f"""
            ğŸ“Š æ ‡æ³¨ç»Ÿè®¡ï¼š
            - æ€»è¯æ•°ï¼š{len(df_all)}
            - å·²æ ‡æ³¨è¯æ•°ï¼š{len(df_labeled)}
            - æ ‡æ³¨ç‡ï¼š{(len(df_labeled)/len(df_all)*100):.1f}%
            """)
            
            # åŒæ ·æä¾›JSONæ ¼å¼
            col3, col4 = st.columns(2)
            
            with col3:
                if not df_all.empty:
                    json_str_all = df_all.to_json(orient='records', force_ascii=False, indent=2)
                    st.download_button(
                        label="ä¸‹è½½å…¨éƒ¨æ ‡æ³¨æ•°æ®(JSON)",
                        data=json_str_all.encode('utf-8'),
                        file_name="annotations_all.json",
                        mime="application/json"
                    )
            
            with col4:
                if not df_labeled.empty:
                    json_str_labeled = df_labeled.to_json(orient='records', force_ascii=False, indent=2)
                    st.download_button(
                        label="ä¸‹è½½å·²æ ‡æ³¨æ•°æ®(JSON)",
                        data=json_str_labeled.encode('utf-8'),
                        file_name="annotations_labeled.json",
                        mime="application/json"
                    )

def export_danmu_analysis(df, video_title):
    """
    å¯¼å‡ºå¼¹å¹•åˆ†æç»“æœä¸ºCSVæ–‡ä»¶
    
    Args:
        df: åŒ…å«å¼¹å¹•åˆ†æç»“æœçš„DataFrame
        video_title: è§†é¢‘æ ‡é¢˜ï¼Œç”¨äºæ–‡ä»¶å‘½å
    """
    try:
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆç§»é™¤ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦ï¼‰
        safe_title = re.sub(r'[\\/*?:"<>|]', "", video_title)
        filename = f"{safe_title}_å¼¹å¹•åˆ†æ.csv"
        
        # è½¬æ¢ä¸ºCSVå¹¶ç¼–ç 
        csv_data = df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
        
        # æä¾›ä¸‹è½½æŒ‰é’®
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½å¼¹å¹•åˆ†æç»“æœ",
            data=csv_data,
            file_name=filename,
            mime="text/csv",
            help="ä¸‹è½½å®Œæ•´çš„å¼¹å¹•åˆ†ææ•°æ®"
        )
        
    except Exception as e:
        st.error(f"å¯¼å‡ºæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
