import streamlit as st
import pandas as pd
import numpy as np
import re
from collections import Counter
import jieba
import io
import requests
from snownlp import SnowNLP
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import base64
import tempfile
import copy

from common import generate_wordcloud,count_word_frequency,count_characters,split_words


example_text="äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿåƒäººç±»ä¸€æ ·æ€è€ƒå’Œå­¦ä¹ çš„æ™ºèƒ½æœºå™¨ã€‚AIæŠ€æœ¯åŒ…æ‹¬æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼‰å’Œè®¡ç®—æœºè§†è§‰ï¼ˆComputer Visionï¼‰ç­‰ã€‚éšç€ç§‘æŠ€çš„è¿›æ­¥ï¼ŒAIåœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ï¼ˆAutonomous Drivingï¼‰ã€åŒ»ç–—è¯Šæ–­ï¼ˆMedical Diagnosisï¼‰å’Œæ™ºèƒ½å®¢æœï¼ˆIntelligent Customer Serviceï¼‰ç­‰ã€‚AIçš„å¿«é€Ÿå‘å±•ä¸ä»…æ”¹å˜äº†æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ï¼Œä¹Ÿå¼•å‘äº†å…³äºä¼¦ç†å’Œéšç§çš„å¹¿æ³›è®¨è®ºã€‚æœªæ¥ï¼ŒAIæœ‰æœ›åœ¨æ•™è‚²ã€é‡‘èã€åˆ¶é€ ä¸šç­‰æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨ç¤¾ä¼šçš„è¿›ä¸€æ­¥å‘å±•ã€‚AIçš„æ½œåŠ›æ˜¯æ— é™çš„ï¼Œå®ƒä¸ä»…å¯ä»¥æé«˜ç”Ÿäº§æ•ˆç‡ï¼Œè¿˜å¯ä»¥é€šè¿‡åˆ†æå¤§é‡æ•°æ®æ¥æä¾›æ›´å¥½çš„å†³ç­–æ”¯æŒã€‚éšç€AIç®—æ³•çš„ä¸æ–­ä¼˜åŒ–å’Œè®¡ç®—èƒ½åŠ›çš„æå‡ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…AIåœ¨è§£å†³å¤æ‚é—®é¢˜å’Œåˆ›æ–°æ–¹é¢å¸¦æ¥æ›´å¤šçªç ´ã€‚"

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="è¯­è¨€åˆ†æå·¥å…·",
    page_icon="ğŸ“š",
    layout="wide"
)

# å¯¼èˆªæ 
st.sidebar.title('å¯¼èˆªæ ')
page = st.sidebar.radio(
    'é€‰æ‹©é¡µé¢',
    ['é¦–é¡µ', 'Bç«™å¼¹å¹•åˆ†æ', 'è¯­æ–™æ¸…æ´—', 'è¯é¢‘ç»Ÿè®¡ä¸è¯äº‘å›¾', 'è¯­æ–™èµ„æºæ•´åˆ']
)

# é¦–é¡µ
if page == 'é¦–é¡µ':
    st.title('è¯­è¨€åˆ†æå·¥å…· ğŸ“š')
    
    st.write("""
    æœ¬å·¥å…·æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
    - ğŸ¬ Bç«™å¼¹å¹•åˆ†æï¼šæ”¯æŒBç«™è§†é¢‘å¼¹å¹•è·å–å’Œåˆ†æï¼ŒåŒ…å«è¯é¢‘ç»Ÿè®¡ã€æƒ…æ„Ÿåˆ†æã€è¯äº‘å›¾ç­‰åŠŸèƒ½
    - ğŸ§¹ è¯­æ–™æ¸…æ´—ï¼šæä¾›æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯åŠŸèƒ½
    - ğŸ“Š è¯é¢‘ç»Ÿè®¡ä¸è¯äº‘å›¾ï¼šå¯¹æ–‡æœ¬è¿›è¡Œè¯é¢‘ç»Ÿè®¡åˆ†æå¹¶ç”Ÿæˆè¯äº‘å›¾
    
    è¯·ä½¿ç”¨å·¦ä¾§å¯¼èˆªæ é€‰æ‹©æ‰€éœ€åŠŸèƒ½ã€‚
    """)
    
    # å±•ç¤ºä¸€äº›ç¤ºä¾‹æˆ–ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ’¡ ä½¿ç”¨è¯´æ˜"):
        st.write("""
        1. æ‰€æœ‰åŠŸèƒ½éƒ½æ”¯æŒæ–‡ä»¶å¯¼å…¥å¯¼å‡º
        2. æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼šTXT, CSV, XLSX
        3. å»ºè®®å•æ¬¡å¤„ç†æ–‡æœ¬å¤§å°ä¸è¶…è¿‡10MB
        """)
    
    # æ·»åŠ é¦–é¡µå›¾ç‰‡ï¼Œè®¾ç½®å®½åº¦ä¸º800åƒç´ 
    st.image('static/LPT.png', width=800)


# Bç«™å¼¹å¹•åˆ†æéƒ¨åˆ†
elif page == 'Bç«™å¼¹å¹•åˆ†æ':
    st.title('Bç«™å¼¹å¹•åˆ†æ ğŸ¬')
    
    # æ·»åŠ åˆ·æ–°æŒ‰é’®
    refresh_button = st.button('åˆ·æ–°', type='primary')
    if refresh_button:
        st.session_state['video_url'] = ""
    
    # è¾“å…¥Bç«™è§†é¢‘URL
    video_url = st.text_input('è¯·è¾“å…¥Bç«™è§†é¢‘URL:', key='video_url')
    if video_url:
        try:
            # è·å–è§†é¢‘ä¿¡æ¯
            headers = {
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'referer': 'https://www.bilibili.com'
            }
            
            response = requests.get(video_url, headers=headers)
            response.raise_for_status()
            
            # è·å–cid
            cid_match = re.search(r'"cid":(\d+)', response.text)
            if not cid_match:
                st.error("æ— æ³•åœ¨é¡µé¢ä¸­æ‰¾åˆ°cid")
            else:
                cid = cid_match.group(1)
                
                # è·å–å¼¹å¹•æ•°æ®
                danmaku_url = f'https://api.bilibili.com/x/v1/dm/list.so?oid={cid}'
                response = requests.get(danmaku_url, headers=headers)
                content = response.content.decode('utf-8')
                
                pattern = r'<d p="([0-9.]+),.*?">(.*?)</d>'
                matches = re.findall(pattern, content)
                
                if not matches:
                    st.warning("æœªæ‰¾åˆ°å¼¹å¹•")
                else:
                    danmaku_list = []
                    for time_str, text in matches:
                        seconds = float(time_str)
                        minutes = int(seconds // 60)
                        remaining_seconds = int(seconds % 60)
                        formatted_time = f"{minutes:02d}:{remaining_seconds:02d}"
                        
                        danmaku_list.append({
                            'time': formatted_time,
                            'time_seconds': seconds,
                            'text': text.strip()
                        })
                    # æŒ‰æ—¶é—´æ’åº
                    danmaku_list.sort(key=lambda x: x['time_seconds'])
                    
                    # æ˜¾ç¤ºå¼¹å¹•æ•°æ®
                    st.write(f"å…±è·å–åˆ° {len(danmaku_list)} æ¡å¼¹å¹•")
                    
                    # æ˜¾ç¤ºå¼¹å¹•é€‰é¡¹
                    show_danmaku = st.checkbox('æ˜¾ç¤ºå¼¹å¹•å†…å®¹')
                    if show_danmaku:
                        display_count = st.number_input('æ˜¾ç¤ºå¼¹å¹•æ•°é‡', 
                                                      min_value=1, 
                                                      max_value=len(danmaku_list),
                                                      value=min(10, len(danmaku_list)))
                        
                        # æ˜¾ç¤ºæŒ‡å®šæ•°é‡çš„å¼¹å¹•
                        st.write('å¼¹å¹•å†…å®¹:')
                        with st.expander("ç‚¹å‡»å±•å¼€æŸ¥çœ‹å¼¹å¹•"):
                            container = st.container()
                            scroll_height = min(400, display_count * 50)  # æ ¹æ®æ˜¾ç¤ºæ•°é‡åŠ¨æ€è°ƒæ•´é«˜åº¦
                            with container:
                                for item in danmaku_list[:display_count]:
                                    st.markdown(f"<div style='margin-bottom:10px'>{item['time']}: {item['text']}</div>", 
                                              unsafe_allow_html=True)
                            # è®¾ç½®å®¹å™¨çš„æœ€å¤§é«˜åº¦å’Œæ»šåŠ¨
                            st.markdown(f"""
                                <style>
                                    [data-testid="stExpander"] div.stMarkdown {{
                                        max-height: {scroll_height}px;
                                        overflow-y: scroll;
                                    }}
                                </style>
                                """, unsafe_allow_html=True)
                    # åˆ†æé€‰é¡¹
                    analysis_type = st.multiselect(
                        'é€‰æ‹©åˆ†æç±»å‹',
                        ['è¯é¢‘ç»Ÿè®¡', 'æƒ…æ„Ÿåˆ†æ', 'è¯äº‘å›¾', 'æ—¶é—´åˆ†å¸ƒ']
                    )
                    
                    if 'è¯é¢‘ç»Ÿè®¡' in analysis_type:
                        st.subheader('ğŸ“Š è¯é¢‘ç»Ÿè®¡åˆ†æ')
                        with st.spinner('æ­£åœ¨è¿›è¡Œè¯é¢‘ç»Ÿè®¡...'):
                            texts = [item['text'] for item in danmaku_list]
                            count_word_frequency(' '.join(texts))
                            
                    if 'æƒ…æ„Ÿåˆ†æ' in analysis_type:
                        st.subheader('ğŸ˜Š æƒ…æ„Ÿåˆ†æ')
                        st.info('æ³¨æ„: å½“å‰æƒ…æ„Ÿåˆ†ææ¨¡å‹ä»æœ‰å¾…æå‡,åˆ†æç»“æœä»…ä¾›å‚è€ƒã€‚')
                        with st.spinner('æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ...'):
                            texts = [item['text'] for item in danmaku_list]
                            sentiments = []
                            danmaku_sentiments = []  # å­˜å‚¨å¼¹å¹•å’Œå¯¹åº”çš„æƒ…æ„Ÿå€¼
                            for text in texts:
                                try:
                                    sentiment = SnowNLP(text).sentiments
                                    sentiments.append(sentiment)
                                    danmaku_sentiments.append({'text': text, 'sentiment': sentiment})
                                except:
                                    continue
                            
                            # å¯¹å¼¹å¹•æŒ‰æƒ…æ„Ÿåˆ†ç±»
                            positive = [d['text'] for d in danmaku_sentiments if d['sentiment'] > 0.6]
                            neutral = [d['text'] for d in danmaku_sentiments if 0.4 <= d['sentiment'] <= 0.6]
                            negative = [d['text'] for d in danmaku_sentiments if d['sentiment'] < 0.4]
                            
                            sentiment_dist = {
                                'ç§¯æ': len(positive),
                                'ä¸­æ€§': len(neutral),
                                'æ¶ˆæ': len(negative)
                            }
                            
                            st.write('æƒ…æ„Ÿåˆ†æç»“æœ:')
                            sentiment_df = pd.DataFrame([sentiment_dist]).T
                            sentiment_df.columns = ['æ•°é‡']
                            st.dataframe(sentiment_df)

                            # æ·»åŠ æ˜¾ç¤ºå„ç±»æƒ…æ„Ÿå¼¹å¹•çš„é€‰é¡¹
                            show_examples = st.checkbox('æ˜¾ç¤ºæƒ…æ„Ÿåˆ†ç±»å¼¹å¹•ç¤ºä¾‹')
                            if show_examples:
                                display_count = st.number_input('æ¯ç±»æ˜¾ç¤ºæ¡æ•°', min_value=1, value=5)
                                
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.write('ğŸ˜Š ç§¯æå¼¹å¹•:')
                                    for text in positive[:display_count]:
                                        st.text(text)
                                with col2:
                                    st.write('ğŸ˜ ä¸­æ€§å¼¹å¹•:')
                                    for text in neutral[:display_count]:
                                        st.text(text)
                                with col3:
                                    st.write('ğŸ˜ æ¶ˆæå¼¹å¹•:')
                                    for text in negative[:display_count]:
                                        st.text(text)
                    if 'è¯äº‘å›¾' in analysis_type:
                        st.subheader('â˜ï¸ è¯äº‘å›¾ç”Ÿæˆ')
                        with st.spinner('æ­£åœ¨ç”Ÿæˆè¯äº‘å›¾...'):
                            texts = [item['text'] for item in danmaku_list]
                            generate_wordcloud(' '.join(texts))
                            
                    if 'æ—¶é—´åˆ†å¸ƒ' in analysis_type:
                        st.subheader('ğŸ“ˆ æ—¶é—´åˆ†å¸ƒåˆ†æ')
                        with st.spinner('æ­£åœ¨åˆ†ææ—¶é—´åˆ†å¸ƒ...'):
                            times = [item['time_seconds'] for item in danmaku_list]
                            fig, ax = plt.subplots(figsize=(12, 6))
                            ax.hist(times, bins=20, color='skyblue', edgecolor='black')
                            ax.set_xlabel('Video Time (seconds)', fontsize=12, fontname='Times New Roman')
                            ax.set_ylabel('Number of Comments', fontsize=12, fontname='Times New Roman')
                            ax.set_title('Time Distribution of Comments', fontsize=14, pad=15, fontname='Times New Roman')
                            ax.grid(True, linestyle='--', alpha=0.7)
                            plt.tight_layout()
                            st.pyplot(fig)
                            
                            # æä¾›ä¸‹è½½æ—¶é—´åˆ†å¸ƒå›¾çš„é€‰é¡¹
                            img = io.BytesIO()
                            plt.savefig(img, format='pdf')
                            img.seek(0)
                            
                            st.download_button(
                                label="ä¸‹è½½æ—¶é—´åˆ†å¸ƒå›¾",
                                data=img,
                                file_name="time_distribution.pdf",
                                mime="application/pdf"
                            )
                            plt.close()
                    # å¯¼å‡ºé€‰é¡¹
                    export_format = st.selectbox(
                        'ğŸ’¾ é€‰æ‹©å¼¹å¹•å¯¼å‡ºæ ¼å¼',
                        ['CSV', 'Excel']
                    )
                    
                    if export_format == 'CSV':
                        df = pd.DataFrame(danmaku_list)
                        csv = df.to_csv(index=False).encode('utf-8-sig')
                        st.download_button(
                            label="ä¸‹è½½CSVæ–‡ä»¶",
                            data=csv,
                            file_name="danmaku.csv",
                            mime="text/csv"
                        )
                    else:
                        df = pd.DataFrame(danmaku_list)
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp:
                            df.to_excel(tmp.name, index=False)
                            with open(tmp.name, 'rb') as f:
                                excel_data = f.read()
                            st.download_button(
                                label="ä¸‹è½½Excelæ–‡ä»¶",
                                data=excel_data,
                                file_name="danmaku.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                            
        except Exception as e:
            st.error(f"è·å–å¼¹å¹•å¤±è´¥: {str(e)}")

# è¯­æ–™æ¸…æ´—éƒ¨åˆ†
elif page == 'è¯­æ–™æ¸…æ´—':
    st.title('è¯­æ–™æ¸…æ´— ğŸ§¹')
    
    if 'ç¤ºä¾‹æ–‡æœ¬' not in st.session_state:
        st.session_state['ç¤ºä¾‹æ–‡æœ¬'] = ""
        
        # æ·»åŠ åˆ·æ–°æŒ‰é’®
    refresh_button = st.button('åˆ·æ–°', type='primary')
    if refresh_button:
        st.session_state['ç¤ºä¾‹æ–‡æœ¬'] = ""
        text_to_clean = ""


    if st.button('ç”Ÿæˆç¤ºä¾‹æ–‡æœ¬'):
        st.session_state['ç¤ºä¾‹æ–‡æœ¬'] = example_text
        text_to_clean = st.session_state['ç¤ºä¾‹æ–‡æœ¬']
    
    if st.session_state['ç¤ºä¾‹æ–‡æœ¬']:
        st.write('ç”Ÿæˆçš„ç¤ºä¾‹æ–‡æœ¬:')
        st.text_area('ç¤ºä¾‹æ–‡æœ¬', st.session_state['ç¤ºä¾‹æ–‡æœ¬'], height=200)
        text_to_clean = st.session_state['ç¤ºä¾‹æ–‡æœ¬']
    else:
        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_file = st.file_uploader("ä¸Šä¼ è¦åˆ†æçš„æ–‡ä»¶", type=['txt', 'csv'])
        
        if uploaded_file is not None:
            text_to_clean = uploaded_file.read().decode()
        else:
            text_to_clean = st.text_area('æˆ–ç›´æ¥è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬:', height=200)
    
    if text_to_clean:
        # æ¸…æ´—é€‰é¡¹
        col1, col2 = st.columns(2)
        with col1:
            remove_punct = st.checkbox('åˆ é™¤æ ‡ç‚¹ç¬¦å·')
            remove_numbers = st.checkbox('åˆ é™¤æ•°å­—')
        with col2:
            to_lowercase = st.checkbox('è½¬æ¢ä¸ºå°å†™')
            remove_spaces = st.checkbox('åˆ é™¤å¤šä½™ç©ºæ ¼')
        
        # ä½¿ç”¨session_stateæ¥ä¿å­˜æ¸…æ´—åçš„æ–‡æœ¬
        if 'cleaned_text' not in st.session_state:
            st.session_state.cleaned_text = None
            
        if st.button('å¼€å§‹æ¸…æ´—'):
            # æ‰§è¡Œæ¸…æ´—
            cleaned_text = text_to_clean
            if remove_punct:
                cleaned_text = re.sub(r'[^\w\s]', '', cleaned_text)
            if remove_numbers:
                cleaned_text = re.sub(r'\d+', '', cleaned_text)
            if to_lowercase:
                cleaned_text = cleaned_text.lower()
            if remove_spaces:
                cleaned_text = ' '.join(cleaned_text.split())
            st.session_state.cleaned_text = cleaned_text
        
        if st.session_state.cleaned_text is not None:
            st.subheader('æ¸…æ´—åçš„æ–‡æœ¬:')
            st.text_area('ç»“æœ', st.session_state.cleaned_text, height=200)
            
            # æ·»åŠ åˆ†è¯åŠŸèƒ½
            if st.button('è¿›è¡Œåˆ†è¯'):
                split_words(st.session_state.cleaned_text)
            
            # ä¿æŒç°æœ‰çš„ä¸‹è½½åŠŸèƒ½
            tet_res = st.session_state.cleaned_text.encode()
            st.download_button(
                label="ä¸‹è½½æ¸…æ´—åçš„æ–‡æœ¬", 
                data=tet_res,
                file_name="cleaned_text.txt",
                mime="text/plain"
            )

# è¯é¢‘ç»Ÿè®¡ä¸è¯äº‘å›¾éƒ¨åˆ†ï¼ˆåŸè¯­è¨€åˆ†æéƒ¨åˆ†ï¼‰
elif page == 'è¯é¢‘ç»Ÿè®¡ä¸è¯äº‘å›¾':
    st.title('è¯é¢‘ç»Ÿè®¡ä¸è¯äº‘å›¾ğŸ“Š')
    
    if 'ç¤ºä¾‹æ–‡æœ¬' not in st.session_state:
        st.session_state['ç¤ºä¾‹æ–‡æœ¬'] = ""
        
        # æ·»åŠ åˆ·æ–°æŒ‰é’®
    refresh_button = st.button('åˆ·æ–°', type='primary')
    if refresh_button:
        st.session_state['ç¤ºä¾‹æ–‡æœ¬'] = ""
        analysis_text = ""


    if st.button('ç”Ÿæˆç¤ºä¾‹æ–‡æœ¬'):
        st.session_state['ç¤ºä¾‹æ–‡æœ¬'] = example_text
        analysis_text = st.session_state['ç¤ºä¾‹æ–‡æœ¬']
    
    if st.session_state['ç¤ºä¾‹æ–‡æœ¬']:
        st.write('ç”Ÿæˆçš„ç¤ºä¾‹æ–‡æœ¬:')
        st.text_area('ç¤ºä¾‹æ–‡æœ¬', st.session_state['ç¤ºä¾‹æ–‡æœ¬'], height=200)
        analysis_text = st.session_state['ç¤ºä¾‹æ–‡æœ¬']
    else:
        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_file = st.file_uploader("ä¸Šä¼ è¦åˆ†æçš„æ–‡ä»¶", type=['txt', 'csv'])
        
        if uploaded_file is not None:
            analysis_text = uploaded_file.read().decode()
        else:
            analysis_text = st.text_area('æˆ–ç›´æ¥è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬:', height=200)
    

    if analysis_text:

        # åˆ†æé€‰é¡¹
        analysis_type = st.multiselect(
            'é€‰æ‹©åˆ†æç±»å‹',
            ['è¯é¢‘ç»Ÿè®¡', 'å­—ç¬¦ç»Ÿè®¡', 'è¯äº‘å›¾']
        )
        
        if 'è¯é¢‘ç»Ÿè®¡' in analysis_type:
            st.subheader('ğŸ“Š è¯é¢‘ç»Ÿè®¡åˆ†æ')
            
            with st.spinner('æ­£åœ¨è¿›è¡Œè¯é¢‘ç»Ÿè®¡...'):
                count_word_frequency(analysis_text)
        if 'å­—ç¬¦è®¡' in analysis_type:
            st.subheader('ğŸ“ å­—ç¬¦ç»Ÿè®¡åˆ†æ')
            with st.spinner('æ­£åœ¨è¿›è¡Œå­—ç¬¦ç»Ÿè®¡...'):
                count_characters(analysis_text)

        
        if 'è¯äº‘å›¾' in analysis_type:
            st.subheader('â˜ï¸ è¯äº‘å›¾ç”Ÿæˆ')
            with st.spinner('æ­£åœ¨ç”Ÿæˆè¯äº‘å›¾...'):
                generate_wordcloud(analysis_text)

# è¯­æ–™èµ„æºæ•´åˆéƒ¨åˆ†
elif page == 'è¯­æ–™èµ„æºæ•´åˆ':
    st.title('è¯­æ–™èµ„æºæ•´åˆ ğŸ“š')
    
    st.markdown("""
    <div style='text-align: center; padding: 10px; color: #566573;'>
        ä¸ºè¯­è¨€ç ”ç©¶æä¾›ä¸°å¯Œçš„è¯­æ–™æ¥æº
    </div>
    """, unsafe_allow_html=True)
    
    # åˆ†ç±»å±•ç¤º
    categories = {
        "ç¤¾äº¤åª’ä½“è¯­æ–™": [
            {
                "name": "Twitter æ•°æ®é›†",
                "type": "æ¨æ–‡æ–‡æœ¬ã€è¯„è®ºã€è½¬å‘ã€ç‚¹èµæ•°",
                "usage": "è¯­è¨€æµè¡Œè¶‹åŠ¿ã€ç½‘ç»œè¯­ä½“åˆ†æã€æƒ…æ„Ÿå˜åŒ–ç ”ç©¶",
                "link": "https://developer.twitter.com/en/docs"
            },
            {
                "name": "Reddit è®¨è®ºæ•°æ®",
                "type": "ä¸»é¢˜å¸–ã€è¯„è®ºã€æŠ•ç¥¨æ•°æ®",
                "usage": "ç¤¾åŒºè®¨è®ºè¯­è¨€åˆ†æã€ç”¨æˆ·äº’åŠ¨è¯­æ–™ç ”ç©¶",
                "link": "https://www.reddit.com/dev/api/"
            }
        ],
        "å­¦æœ¯æ–‡çŒ®è¯­æ–™": [
            {
                "name": "PubMed Central",
                "type": "åŒ»å­¦æ–‡çŒ®çš„æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯",
                "usage": "åŒ»å­¦æœ¯è¯­ç ”ç©¶ã€å­¦æœ¯å†™ä½œåˆ†æ",
                "link": "https://www.ncbi.nlm.nih.gov/pmc/"
            },
            {
                "name": "ACL Anthology",
                "type": "å­¦æœ¯è®ºæ–‡çš„æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯",
                "usage": "è¯­è¨€å­¦ç ”ç©¶ã€å­¦æœ¯å†™ä½œåˆ†æ",
                "link": "https://www.aclweb.org/anthology/"
            }
        ]
    }
    
    for category, apis in categories.items():
        st.subheader(category)
        for api in apis:
            st.markdown(f"**{api['name']}**")
            st.write(f"è¯­æ–™ç±»å‹ï¼š{api['type']}")
            st.write(f"ç”¨é€”ï¼š{api['usage']}")
            st.write(f"[è®¿é—®API]({api['link']})")
            st.markdown("---")
